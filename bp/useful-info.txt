Author of the algorithm: Paul Werbos

***To read: https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf***
page 733, chapter 18.7.4 (Learning in multilayer networks)

Summarized:

The major complication comes from the addition of hidden layers to the network.
Whereas the error y − hw at the output layer is clear, the error at the hidden layers seems
mysterious because the training data do not say what value the hidden nodes should have.
Fortunately, it turns out that we can back-propagate the error from the output layer to theBACK-PROPAGATION
hidden layers. The back-propagation process emerges directly from a derivation of the overall
error gradient. First, we will describe the process with an intuitive justification; then, we will
show the derivation.

To update the connections between the input units and the hidden units, we need to define a
quantity analogous to the error term for output nodes. Here is where we do the error backpropagation.
The idea is that hidden node j is “responsible” for some fraction of the error Δk
in each of the output nodes to which it connects. Thus, the Δk values are divided according
to the strength of the connection between the hidden node and the output node and are propagated
back to provide the Δj values for the hidden layer. The propagation rule for the Δ
values is the following:

Δj = g′(inj ) ∑kw(j,k)Δk.


## Advantages:
<ul>
    <li>
    Applicability: Backpropagation can be applied to a wide range of problems, making it a versatile algorithm for training neural networks.
    It's commonly used in applications like image recognition, natural language processing, and other complex tasks.
    </li>
    <li>
    Efficiency: Compared to some other methods, backpropagation is often more computationally efficient for large neural networks.
    It computes the gradient of the loss function with respect to the weights of the network for a single input-output example, and does so efficiently, unlike some naive approaches.
    </li>
    <li>
    Deeper Insights: Backpropagation helps provide insights into the workings of the neural network by identifying the contribution of each neuron to the final output error.
    </li>
</ul>

## Disadvantages:

<ul>
    <li>
     Local Minima: The biggest disadvantage of backpropagation is that it can get stuck in local minima while trying to minimize the loss function.
     However, in practice, especially in high dimensional spaces, this is less of a problem than often assumed
    </li>
    <li>
     Vanishing/Exploding Gradients: Backpropagation can suffer from the vanishing or exploding gradient problems,
     especially in deep neural networks and recurrent neural networks. This is when gradients become either too small or too large, making it difficult for the network to learn effectively.
    </li>
    <li>
        Requires Labelled Data: Backpropagation requires labelled data to train the network,
        which can be a disadvantage in scenarios where such data is scarce or expensive to obtain.
    </li>
     <li>
     Difficulty in Understanding: The internal workings of a neural network using backpropagation can be hard to interpret. As networks grow in size and complexity,
     understanding the exact influence and weight of each input can become challenging, leading to them often being referred to as "black boxes".
    </li>
    <li>
        Parameter Tuning: Neural networks trained with backpropagation often require careful tuning of hyperparameters,
        such as the learning rate, number of layers, number of units per layer, etc. These settings can greatly impact the success of the training process and can require a lot of computational resources to optimize.
    </li>
</ul>

Алгоритм було розроблено Полом Вербосом і називається він "back-propagation". Він дозволяє навчати багатошарові нейронні мережі.
Головна складність полягає у введенні прихованих шарів в мережу.
Помилка на виході мережі зрозуміла, але помилка на прихованих шарах здається таємничою, оскільки навчальні дані не вказують,
яке значення повинні мати приховані вузли. Процес back-propagation виникає безпосередньо з похідної від загального градієнту помилки.

Переваги:

Застосовність: back-propagation можна застосовувати для великої кількості задач, що робить його універсальним алгоритмом для навчання нейронних мереж.
Він часто використовується в застосунках, таких як розпізнавання зображень, обробка природної мови та інші складні завдання.
Ефективність: У порівнянні з деякими іншими методами, back-propagation часто є більш обчислювально ефективним для великих нейронних мереж.
Він обчислює градієнт функції втрати по відношенню до ваг мережі для одного вхідного-вихідного прикладу, і робить це ефективно, на відміну від деяких наївних підходів.
Глибокі уявлення: back-propagation допомагає надати уявлення про роботу нейронної мережі,
ідентифікуючи внесок кожного нейрона у кінцеву помилку виходу.

Недоліки:
1. Локальні мініма: Найбільшою недолікою back-propagation є можливість застрягнути в локальних мінімумах при спробі мінімізувати функцію втрат.
Однак на практиці, особливо в високорозмірних просторах, це менша проблема, ніж часто припускають.
2. Вигасаючі / Вибухаючі градієнти: back-propagation може страждати від проблем вигасання або вибухання градієнтів, особливо в
глибоких нейронних мережах та рекурентних нейронних мережах. Це відбувається, коли градієнти стають або занадто малими,
або занадто великими, що ускладнює ефективне навчання мережі.
3. Вимагає маркованих даних: back-propagation потребує маркованих даних для навчання мережі,
що може бути недоліком в сценаріях, де такі дані рідкісні або дорогі у втрату.
4. Важкість в розумінні: Внутрішні роботи нейронної мережі з використанням back-propagation можуть бути важкими для інтерпретації.
При зростанні розміру і складності мережі розуміння точного впливу та ваги кожного входу може стати складним, що часто призводить до їх називання "чорні скриньки".
5. Налагодження параметрів: Нейронні мережі, навчені з використанням back-propagation, часто потребують
уважного налаштування гіперпараметрів, таких як швидкість навчання, кількість шарів, кількість одиниць на шарі і т. д.
Ці налаштування можуть суттєво вплинути на успішність процесу навчання і можуть вимагати багато обчислювальних ресурсів для оптимізації.